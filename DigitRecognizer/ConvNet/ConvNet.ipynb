{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# inspired by:\n",
    "# http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = pd.read_csv(\"../input/train.csv\").values\n",
    "rawSubmission = pd.read_csv(\"../input/test.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#suffle dataset\n",
    "np.random.shuffle(rawData)\n",
    "\n",
    "trainSamples = rawData.shape[0] # amount of samples in training set\n",
    "testSamples = rawSubmission.shape[0] # amount of samples in test set\n",
    "imgSize = 28 # image size (both width and height)\n",
    "colorChannels = 1 # images are grayscale\n",
    "\n",
    "# reshape and normalize\n",
    "X = rawData[:, 1:].astype(\"float32\").reshape( (trainSamples, imgSize, imgSize, colorChannels) ) / 255.0\n",
    "testX = rawSubmission.astype(\"float32\").reshape( (testSamples, imgSize, imgSize, colorChannels) ) / 255.0\n",
    "\n",
    "Y = np_utils.to_categorical( rawData[:,0].astype(\"int\") )\n",
    "\n",
    "numClasses = Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will use 3 conv layers with 32 filters with size 5x5\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, input_shape=(imgSize, imgSize, colorChannels), \n",
    "                        border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, activation='relu', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, activation='relu', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(numClasses, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_4 (Convolution2D)  (None, 28, 28, 32)    832         convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 28, 28, 32)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 28, 28, 32)    25632       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 14, 14, 32)    0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 14, 14, 32)    0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 14, 14, 32)    25632       dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 7, 7, 32)      0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1568)          0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           803328      flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 512)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 10)            5130        dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 860554\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# stop training if val_loss stop decreasing after 2 epoch\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=2)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/25\n",
      "31500/31500 [==============================] - 368s - loss: 0.3157 - acc: 0.8963 - val_loss: 0.0670 - val_acc: 0.9785\n",
      "Epoch 2/25\n",
      "31500/31500 [==============================] - 366s - loss: 0.0821 - acc: 0.9741 - val_loss: 0.0522 - val_acc: 0.9835\n",
      "Epoch 3/25\n",
      "31500/31500 [==============================] - 371s - loss: 0.0610 - acc: 0.9811 - val_loss: 0.0414 - val_acc: 0.9870\n",
      "Epoch 4/25\n",
      "31500/31500 [==============================] - 372s - loss: 0.0467 - acc: 0.9854 - val_loss: 0.0367 - val_acc: 0.9879\n",
      "Epoch 5/25\n",
      "31500/31500 [==============================] - 380s - loss: 0.0387 - acc: 0.9880 - val_loss: 0.0361 - val_acc: 0.9886\n",
      "Epoch 6/25\n",
      "31500/31500 [==============================] - 365s - loss: 0.0318 - acc: 0.9902 - val_loss: 0.0346 - val_acc: 0.9893\n",
      "Epoch 7/25\n",
      "31500/31500 [==============================] - 364s - loss: 0.0298 - acc: 0.9905 - val_loss: 0.0367 - val_acc: 0.9882\n",
      "Epoch 8/25\n",
      "31500/31500 [==============================] - 378s - loss: 0.0253 - acc: 0.9912 - val_loss: 0.0326 - val_acc: 0.9900\n",
      "Epoch 9/25\n",
      "31500/31500 [==============================] - 383s - loss: 0.0243 - acc: 0.9922 - val_loss: 0.0323 - val_acc: 0.9900\n",
      "Epoch 10/25\n",
      "31500/31500 [==============================] - 384s - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0320 - val_acc: 0.9909\n",
      "Epoch 11/25\n",
      "31500/31500 [==============================] - 385s - loss: 0.0220 - acc: 0.9933 - val_loss: 0.0316 - val_acc: 0.9904\n",
      "Epoch 12/25\n",
      "31500/31500 [==============================] - 386s - loss: 0.0167 - acc: 0.9951 - val_loss: 0.0308 - val_acc: 0.9910\n",
      "Epoch 13/25\n",
      "31500/31500 [==============================] - 386s - loss: 0.0166 - acc: 0.9948 - val_loss: 0.0339 - val_acc: 0.9907\n",
      "Epoch 14/25\n",
      "31500/31500 [==============================] - 386s - loss: 0.0161 - acc: 0.9950 - val_loss: 0.0321 - val_acc: 0.9910\n",
      "Epoch 15/25\n",
      "31500/31500 [==============================] - 369s - loss: 0.0148 - acc: 0.9952 - val_loss: 0.0326 - val_acc: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1fa971f98>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_split = 0.25 - split oyr dataset: train on 75% of samples and validate on 25%\n",
    "model.fit(X, Y, validation_split = 0.25, nb_epoch=epochs, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally save model to .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('convnet_9910.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 104s   \n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict_classes(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = np.column_stack( (np.arange(1, predicted.shape[0]+1), predicted) )\n",
    "np.savetxt('convnet_v1.csv', submission, delimiter=',', header = 'ImageId,Label', fmt=\"%d\", comments='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
